{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "##install: py -m pip install box2d\n",
    "##install: py -m pip install pyvirtualdisplay\n",
    "##install: py -m pip install torch===1.6.0 torchvision===0.7.0 -f https://download.pytorch.org/whl/torch_stable.html\n",
    "import random\n",
    "import gym\n",
    "import torch ##scientific computing framework with \n",
    "             ##wide support for machine learning algorithms that puts GPUs first\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from collections import deque, namedtuple\n",
    "import matplotlib.pyplot as plt ##Graphs\n",
    "import os\n",
    "##from pyvirtualdisplay import Display\n",
    "##display = Display(visible=0, size=(900, 500))\n",
    "##display.start()\n",
    "\n",
    "is_ipython = 'inline' in plt.get_backend() ##To make figure in notebook\n",
    "if is_ipython:\n",
    "    from IPython import display\n",
    "\n",
    "plt.ion() ##used to turn on interactive mode\n",
    "\n",
    "EPISODES = 2000\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'dqn' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-13-7ff336bd05c8>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     42\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mscores\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     43\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 44\u001b[1;33m \u001b[0mscores\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdqn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     45\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     46\u001b[0m \u001b[1;31m# plot the scores with graph\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'dqn' is not defined"
     ]
    }
   ],
   "source": [
    "class QNetwork(nn.Module):\n",
    "    ##Actor (Policy) Model.\n",
    "\n",
    "    def __init__(self, state_size, action_size, seed, fc1_units=64, fc2_units=64):\n",
    "        ##Initialize parameters and build model.\n",
    "        ##Params\n",
    "        ##======\n",
    "        ##    state_size (int): Dimension of each state\n",
    "        ##    action_size (int): Dimension of each action\n",
    "        ##    seed (int): Random seed\n",
    "        ##    fc1_units (int): Number of nodes in first hidden layer\n",
    "        ##    fc2_units (int): Number of nodes in second hidden layer\n",
    "        ##\n",
    "        super(QNetwork, self).__init__()\n",
    "        self.seed = torch.manual_seed(seed)\n",
    "        self.fc1 = nn.Linear(state_size, fc1_units)\n",
    "        self.fc2 = nn.Linear(fc1_units, fc2_units)\n",
    "        self.fc3 = nn.Linear(fc2_units, action_size)\n",
    "\n",
    "    def forward(self, state):\n",
    "        ##Build a network that maps state -> action values.\n",
    "        x = F.relu(self.fc1(state))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        return self.fc3(x)\n",
    "\n",
    "\n",
    "class DQNAgent:\n",
    "    def __init__(self, state_size, action_size):\n",
    "        self.state_size = state_size\n",
    "        self.action_size = action_size\n",
    "        \n",
    "        self.gamma = 0.95    ## discount rate\n",
    "        self.max_t: 1000 ## maximum number of timesteps per episode\n",
    "        self.epsilon = 1.0  ## exploration rate, starting value of epsilon, for epsilon-greedy action selection \n",
    "        self.epsilon_min = 0.01 ##minimum value of epsilon\n",
    "        self.epsilon_decay = 0.995 ##multiplicative factor (per episode) for decreasing epsilon\n",
    "        self.learning_rate = 0.001\n",
    "        ##TAU = 1e-3              # for soft update of target parameters\n",
    "        LR = 5e-4               # learning rate \n",
    "        UPDATE_EVERY = 4        # how often to update the network\n",
    "        self.model = self._build_model()\n",
    "        \n",
    "        self.qnetwork_local = QNetwork(state_size, action_size, seed).to(device)\n",
    "        self.qnetwork_target = QNetwork(state_size, action_size, seed).to(device)\n",
    "        self.optimizer = optim.Adam(self.qnetwork_local.parameters(), lr=LR)\n",
    "    \n",
    "    def _build_model(self):\n",
    "        # Neural Net for Deep-Q learning Model\n",
    "        model = keras.models.Sequential()\n",
    "        model.add(keras.layers.Dense(24,input_dim=self.state_size,activation=\"relu\"))\n",
    "        model.add(keras.layers.Dense(24,activation=\"relu\"))\n",
    "        model.add(keras.layers.Dense(self.action_size,activation=\"linear\"))\n",
    "        model.compile(loss='mse',\n",
    "                      optimizer=keras.optimizers.Adam(lr=self.learning_rate))\n",
    "        return model\n",
    "\n",
    "    def memorize(self, state, action, reward, next_state, done):\n",
    "        self.memory.append((state, action, reward, next_state, done))\n",
    "\n",
    "    def act(self, state):\n",
    "        if np.random.rand() <= self.epsilon:\n",
    "            return random.randrange(self.action_size)\n",
    "        act_values = self.model.predict(state)\n",
    "        return np.argmax(act_values[0])  # returns action\n",
    "\n",
    "    def replay(self, batch_size):\n",
    "        minibatch = random.sample(self.memory, batch_size)\n",
    "        for state, action, reward, next_state, done in minibatch:\n",
    "            target = reward\n",
    "            ## Get max predicted Q values (for next states) from target model\n",
    "            Q_targets_next = self.qnetwork_target(next_states).detach().max(1)[0].unsqueeze(1)\n",
    "            if not done:\n",
    "                # Compute Q targets for current states \n",
    "                Q_targets = rewards + (gamma * Q_targets_next * (1 - dones))\n",
    "                ##target = (reward + self.gamma * np.amax(self.model.predict(next_state)[0]))\n",
    "            \n",
    "            # Compute loss\n",
    "            loss = F.mse_loss(Q_expected, Q_targets)\n",
    "            # Minimize the loss\n",
    "            self.optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            self.optimizer.step()\n",
    "              \n",
    "        \n",
    "            target_f = self.model.predict(state)\n",
    "            target_f[0][action] = target\n",
    "            self.model.fit(state, target_f, epochs=1, verbose=0)\n",
    "        if self.epsilon > self.epsilon_min:\n",
    "            self.epsilon *= self.epsilon_decay\n",
    "\n",
    "    def load(self, name):\n",
    "        self.model.load_weights(name)\n",
    "\n",
    "    def save(self, name):\n",
    "        self.model.save_weights(name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    env = gym.make('LunarLander-v2')\n",
    "    state_size = env.observation_space.shape[0]\n",
    "    action_size = env.action_space.n\n",
    "    agent = DQNAgent(state_size, action_size)\n",
    "    \n",
    "    \n",
    "    scores = []                        # list containing scores from each episode\n",
    "        ##self.memory = deque(maxlen=100)\n",
    "        scores_window = deque(maxlen=100)  # last 100 scores\n",
    "        eps = self.epsilon                    # initialize epsilon\n",
    "        for i_episode in range(1, EPISODES+1):\n",
    "            state = env.reset()\n",
    "            score = 0\n",
    "            for t in range(self.max_t):\n",
    "                action = agent.act(state, eps)\n",
    "                next_state, reward, done, _ = env.step(action)\n",
    "                agent.step(state, action, reward, next_state, done)\n",
    "                state = next_state\n",
    "                score += reward\n",
    "                if done:\n",
    "                    break \n",
    "            scores_window.append(score)       # save most recent score\n",
    "            scores.append(score)              # save most recent score\n",
    "            eps = max(self.epsilon_min, epsilon_decay*eps) # decrease epsilon\n",
    "           \n",
    "            print('\\rEpisode {}\\tAverage Score: {:.2f}'.format(i_episode, np.mean(scores_window)), end=\"\")\n",
    "            \n",
    "            if i_episode % 100 == 0:\n",
    "                print('\\rEpisode {}\\tAverage Score: {:.2f}'.format(i_episode, np.mean(scores_window)))\n",
    "            if np.mean(scores_window)>=200.0:\n",
    "                print('\\nEnvironment solved in {:d} episodes!\\tAverage Score: {:.2f}'.format(i_episode-100, np.mean(scores_window)))\n",
    "                torch.save(agent.qnetwork_local.state_dict(), 'checkpoint.pth')\n",
    "                break\n",
    "        return scores\n",
    "\n",
    "scores = dqn()\n",
    "\n",
    "# plot the scores with graph \n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111)\n",
    "plt.plot(np.arange(len(scores)), scores)\n",
    "plt.ylabel('Score')\n",
    "plt.xlabel('Episode #')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
